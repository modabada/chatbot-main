의미없는 수업이었다

- 임베딩
	- 원-핫 인코딩
		- 그래프를 N*N 의 테이블에 저장하는 것과 같은 방식, 해당하는 단어는 해당하는 셀에 1 을 써서 저장
		- 단점
			- 공간 낭비가 심함
			- 단어들 간의 관계성을 부여할 수 없음
			- 하나의 단어를 표현하는데 말뭉치의 수 만큼 차원이 존재해 소위 `차원의 저주` 단점
	- 횟수 기반 임베딩
	  단어가 출현한 빈도를 고려하여 인베딩하는 방법
		- 카운터 벡터
			- 단어를 토큰으로 생성하고 각 단어의 출현 빈도수를 이용해 인코딩해서 벡터를 만듦
			  즉, tokenizing 과 벡터화가 동시에 가능함
		- TF-IDF
			- 정보 검색론에서 가중치를 구할 때 사용되는 알고리즘
			- TF(`Term Frequency`) 는 문서 내에서 특정 단어가 출현한 빈도를 의미,
			- DF(`Document Frequency`) 는 한 단어가 전체 문서에서 얼마나 공통적으로 많이 등장하는지 나타내는 값
			- IDF(`Inverse Document Frequency`) 특정 단어가 일반적인 단어 a, the 등과 같다면 TF-IDF 가중치를 낮추어야 한다.
			  따라서 DF 가 클수록 TF-IDF 의 가중치를 낮추기 위해 DF 의 역수를 취한다
			  역수르 취하면 전체 문서 개수가 많아질수록 IDF 도 커지기 때문에 로그를 취해야 한다.
			  하지만 이대 특정 단어가 발생하는 빈도가 0이면 IDF 가 0 이 되기 때문에 분모에 1을 더해 계산하는데 이를 `스무딩` 이라고 한다
			- TF-IDF 는 주로 다음 상황에서 사용된다
				- 키워드 검색을 기반으로 하는 검색 엔진
				- 중요 키워드 분석
				- 검색 엔진에서 검색 결과의 순위를 결정
	- 예측 기반 임베딩
		- 신경망을 이용해 특정 문맥에서 어떤 단어가 나올지 예측하면서 단어를 벡터로 만듦, 대표적으로 워드 투 벡터
			- 워드 투 벡터
				- 주어진 텍스트의 각 단어마다 하나씩일련의 벡터를 출력하는 방식
				- 요소
					- CBOW (강사가 애미없어서 여기에 정렬되는게 맞는지 모르겠음)
						- 단어를 여러 개 나열한 후, 이와 관련된 단어를 추정하는 방식
					- skip-gram (강사가 설명 1도 안함)
			- 페스트 텍스트
				- 자주 사용되지 않는 단어에 대한 안정성 확보
				  워드 투 벡터는 사전에 없는 단어에 대해서는 벡터값을 얻을 수 없고 자주 사용되지 않는 단어에 대해서는 안정성이 낮은 단점을 보완하기 위해 만들어짐
			  - 노이즈에 강하며 새로운 단어는 형태적 유사성을 고려한 백터값을 얻어 자연어 처리 분야에서 많이 사용됨
  - 횟수/예측 기반 임베딩
	  - 글로브

- 트랜스포머 어텐션
	- 어텐션은 주로 언어 번역에서 사용되기 때문에 인코더와 디코더 네트워크를 사용함
		- 입력에 대한 벡터 변환은 인코더에서 처리하고 모든 벡터를 디코더로 보냄
		  이유: 기울기 소멸 문제를 해결하기 위해서
		- 모든 벡터를 전달해 행렬 크기가 커질 수 있는데 이를 위해 소프트맥스 함수로 가중합을 구하고 그 값을 디코더로 전달
		- 가중합만 전달되어도 정보를 많이 전달받은 디코더는 부담일 수밖에 없는데, 디코더는 은닉 상태에 대해 중심적으로 집중 해서 보아야 할 벡터를 소프트맥스 함수로 점수를 매긴 후, 각각을 은닉 상태의 벡터들과 곱함. 그리고 이 은닉 상태를 모두 더해 하나의 값으로 만듦, 즉 어텐션은 모든 벡터 중에서 꼭 살펴보아야 할 벡터들에 집중하겠다는 의미