강화학습
- 마르코프 결정 과정
	- 마르코프 프로세스
		- 어떤 상태가 일정한 간격으로 변하고, 다음 상태는 현재 상태에만 의존하는 확률적인 상태 변화
		- 현재 상태까지의 과정은 상관 없고, 현재 상태만을 보며 다음 상태를 결정
		- 이와 같은 프로세스가 여러 개 묶여 있는 경우를 `마르코프 체인`이라고 함
	- 마르코프 보상 프로세스
		- 마르코프 프로세스에서 각 상태마다 좋고 나쁨이 추가된 확률 모델
		- 어느 시점에서 보상을 받을지가 중요함
		- 현재가치에 이자를 더하면 미개가ㅣ가 됨
		- 상태-가치 함수
			- 상태 s에서 얻을 수 있는 가치
		- 행동-가치 함수
			- 상태 s에서 a라는 행동을 취했을 때 얻을 수 있는 기대값
		- 각 가치함수를 계산하는 방법은 O(n^3) 시간으로 굉장히 큰데 이를 위해 다음과 같은 방법이 제안됨
			- DP
			- 몬테카를로
				- 일부 상태만 확인하며 근사적으로 가치 추정
				- 초기 상태에서 시작해 중간 상태들을 경유해 최종 상태까지 간 후, 최종 보상을 측정하고 방문했던 상태들의 가치를 업데이트
			- 시간차 학습
				- 몬테카를로는 최종상태까지 도달한 수에야 업데이트가 가능하다는 단점이 있는데 시간차 학습은 방문한 즉시 업데이트를 함
				- DP와 몬테카를로의 중간점 정도
			- 함수적 접근 학습
				- 프로그래머가 머리써서 하는 복잡한 DP같은거래요
				- 수업좀 했으면 좋겠다
	- MDP 를 위한 벨만 방정식
		- 벨만 기대 방정식
			- 다음 상태로 이동할 때, 어떤 정책 (policy) 에 따라 행동해야 하는데 이 정책을 고려한 다음 상태로의 이동이 `벨만 기대 방정식`이다
			- 상태-가치함수 및 행동-가치함수 에 대한 계산이 있긴 한데 굳이 이해할 필요는 없음
			- 과정 정리
				1. 처음 에이전트가 접하는 상태 s나 행동 a는 임의의 값으로 설정
				2. 환경과 상호작용 하며 얻은 보상과 상태의 정보들을 이용해 어떤 상태에서 어떤 행동을 취하는지 좋은지를 판단
				3. 이때 최적의 행동을 판단하는 수단이 가치함수 들이고 이를 벨만 기대 방정식을 이용해 업데이트 하며 높은 보상을 얻을 수 있는 상태와 행동을 학습
				4. 2~3 과정속에서 최대 보상을 갇는 행동을 선택하도록 최적화된 policy 를 찾음
		- 벨만 최적 방정식
			- 최적의 가치 함수값을 찾는것이 아닌 **최대의 보상을 얻는 정책** 을 찾는 것이 목표임
	- DP
		- MDP 와 정책을 입력으로 가치함수를 찾아내는 예측 과정
		- MDP 를 입력으로 하여 기존 가치함수를 최적화 하는 것이 컨트롤 과정
		- 가치 함수를 이용해 최적화된 정책을 찾을 수 있음
		- 수업좀 해 시팔
- 큐 러닝
	- 주어진 상태에서 행동을 취했을 경우 받을 수 있는 보상의 기대값을 예측하는 `큐 함수`를 이용해 최적화된 정책을 찾는 강화 학습 기법
	- 즉 여러 실험을 반복해 최적의 정책을 학습함
	- 매 실험에서 각 상태마다 랜덤한 행동을 취함
		- 탐험과 활용의 균형이 중요함
		- 탐험
			- 그리디 알고리즘을 이용해 새로운 좋은 경로를 찾음
			- 탐험의 결과가 늘 최적의 결과가 아니기 때문에 손해가 발생하긴 함
			- 하지만 경험을 통해 더 좋은 길을 찾을 수 있음
		- 활용
			- 현재까지 경험 중 현 상태에서 최대의 보상을 받을 수 있는 행동을 하는 것
	- 단점
		- 상태 개수가 많은 경우 큐-테이블 구축에 한계가 있음
		- 데이터간 상관관계로 학습이 어려움
- 딥 큐 러닝 (Deep Q-Learning)
	- 합성곱 신경망을 이용해 큐-함수 를 학습하는 강화 학습 기법
	- 합성곱층을 깊게 하여 훈련할 때, 큐 값의 정확도를 높이는것을 목표로 함
	- 게임에 많이 쓰임, 언리얼 등의 플러그인도 있음
	- 용어
		- 타깃 큐 네트워크
		- 리플레이 메모리
- 몬테카를로 트리 탐색
	- 모든 트리 노드를 탐색하는 대신, 게임 시뮬레이션으로 가장 가능성이 높아 보이는 방향으로 행동을 결정하는 탐색 방법
	- 즉, 무작위 방법중 가장 승률이 높은 값을 기반으로 시도하는 것